{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with BERT Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though word embeddings and recurrent neural networks such as LSTM have proven to be extremely efficient for various natural language processing tasks such as text classification, there are a few problems with LSTM and CNN based text classification systems.  \n",
    "\n",
    "With LSTM, data can only be read in a sequential manner in one direction. Though bidirectional LSTM solves this problem by reading data in both forward and backward directions, the text is still processed sequentially instead of being processed parallelly. This is where transformer models come into play. Transformer models process the whole text document in parallel, relying on attention mechanism.  \n",
    "\n",
    "In the attention mechanism, instead of processing text sequentially, the text is processed in parallel, which allows the attention systems to assign weightage to important parts of the text in a parallel manner.  \n",
    "\n",
    "Several transformer models have been developed until now. However, in this chapter, you will be studying BERT, which stands for (Bidirectional Encoder Representations from Transformers) developed by Google.  \n",
    "\n",
    "**Why use Transformers (BERT)**  \n",
    "Why should we use BERT over traditional word embeddings and LSTM based neural networks?  \n",
    "\n",
    "BERT or Transformer models are able to generate word representations that capture local context. For instance, with word embeddings, the representation of the word “apple” will be the same even if we talk about “apple I phone” or apple as a fruit. With BERT, a different word representation will be generated. In addition, BERT models divide words into stems and leaf segments.  \n",
    "- For instance, the word “Judgmental” is treated as two separate tokens “Judgement” and “al,” which makes word representation more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Text Classification\n",
    "The BERT model is used for generating text representations, which you can then use with LSTM or CNN networks to build text classification or any other type of model. However, BERT also contains Sequence Classification models, which can be used to classify text.  \n",
    "\n",
    "The steps involved in text classification using BERT sequence models are as follows:\n",
    "1. Generate BERT tokens (use the BERT Tokenizer).\n",
    "2. Convert data into the format that can be used by a BERT model and perform word embeddings using BERT.\n",
    "3. Create a sequence classification model for BERT.\n",
    "4. Train the sequence classification model.\n",
    "5. Evaluate the model on the test set.  \n",
    "\n",
    "For this exercise you can download the IMDB sentiment dataset from [kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenization and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_sent_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_sent ['[CLS]', 'queen', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample_sent= 'Hello, are you judgemental. No I am incremental.'\n",
    "sample_sent= 'queen'\n",
    "\n",
    "# add start and stop tokens\n",
    "sample_sent_plus_special_tokens = '[CLS]' + sample_sent + '[SEP]'\n",
    "\n",
    "tokenized_sent = bert_tokenizer.tokenize(sample_sent_plus_special_tokens)\n",
    "print('tokenized_sent',tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- “Judgmental” is treated as two separate tokens “Judgement” and “al”\n",
    "- “incremental” is treated as three separate tokens “inc” and “rem” and \"ental\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text tokens into the integer format using the **_convert_tokens_to_ids()_** method of the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3035, 102]\n"
     ]
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply padding to the input sentences so that all the sentences have the same length.  \n",
    "Since the maximum sentence length is set to 25 and the input sentence contains 16 tokens, the pad length will be 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "pad_length = max_sent_length - len(input_ids)\n",
    "print(pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 1010, 2024, 2017, 16646, 2389, 1012, 2053, 1045, 2572, 4297, 28578, 21050, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids + ([0] * pad_length)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention masks are a list of 0s and 1s where a 1 is added to the positions that contained original objects, while 0 is added to the padding indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "att_mask = [1] * len(input_ids)\n",
    "att_mask = att_mask + ([0] * pad_length)\n",
    "print(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token type ids are used to separate input sentence from an output sentence in case there are two sentences in the input.  \n",
    "Zeros are added for the index of the first sentence, while a 1 is added for the indexes of the other sentence.  \n",
    "Since we have only a single sentence, we will only create a list of all 0s for token type ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = [0] * max_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the code below create a complete input for BERT models, a dictionary of token ids, token type ids, and attention mask is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': [101, 7592, 1010, 2024, 2017, 16646, 2389, 1012, 2053, 1045, 2572, 4297, 28578, 21050, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "input_for_bert = {\n",
    "    \"token_ids\": input_ids, \n",
    "    \"token_type_ids\": token_type_ids, \n",
    "    \"attention_mask\": att_mask\n",
    "}\n",
    "print(input_for_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic BERT input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _**encode_plus**_ function of the BertTokenizer object can  be used to create the BERT input automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded {'input_ids': [101, 7592, 1010, 2024, 2017, 16646, 2389, 1012, 2053, 1045, 2572, 4297, 28578, 21050, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndah/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_for_bert = bert_tokenizer.encode_plus(\n",
    "    sample_sent, \n",
    "    add_special_tokens = True, \n",
    "    max_length = max_sent_length,\n",
    "    truncation=True,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True\n",
    ")\n",
    "\n",
    "print(\"encoded\", input_for_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification  \n",
    "Training is computationally intensive good to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_hub\n",
    "# !pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.8.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.12.0\n",
      "WARNING:tensorflow:From <ipython-input-13-22f27223ab92>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(\"Version:\", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(‘/gdrive’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv(r\"data/IMDB_Dataset.csv\")\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(document):\n",
    "    return TAG_RE.sub('', document)\n",
    "\n",
    "\n",
    "def clean_text(doc):\n",
    "    document = remove_tags(doc)\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data[\"review_clean\"]=imdb_data[\"review\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "\n",
       "                                        review_clean  \n",
       "0  One of the other reviewers has mentioned that ...  \n",
       "1  A wonderful little production The filming tech...  \n",
       "2  I thought this was wonderful way to spend time...  \n",
       "3  Basically there a family where little boy Jake...  \n",
       "4  Petter Mattei Love in the Time of Money is vis...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (TFBertForSequenceClassification, BertTokenizer)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (40000,),\n",
      "Shape of test data: (10000,)\n"
     ]
    }
   ],
   "source": [
    "X = imdb_data[\"review_clean\"].values\n",
    "y = imdb_data['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training data: {0},\\nShape of test data: {1}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an object of the BertTokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 9.06kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 317kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 660kB/s] \n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 276kB/s]\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lonest_sentence(reviews):\n",
    "    max_length = 0\n",
    "    for review in reviews:\n",
    "        if len(review) > max_length:\n",
    "            max_length = len(review)\n",
    "            \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token=0\n",
    "pad_token_segment_id=0\n",
    "\n",
    "max_length= 128\n",
    "max_length = lonest_sentence(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bert_input(reviews):\n",
    "    \n",
    "    input_ids,attention_masks,token_type_ids=[],[],[]\n",
    "    for review in tqdm(reviews):\n",
    "        bert_inputs = bert_tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    inputs, token_type = bert_inputs[\"input_ids\"], bert_inputs[\"token_type_ids\"]\n",
    "    mask = [1] * len(inputs)\n",
    "    padding_length = max_length - len(inputs)\n",
    "    \n",
    "    inputs = inputs + ([pad_token] * padding_length)\n",
    "    mask = mask + ([0] * padding_length)\n",
    "    token_type = token_type + ([pad_token_segment_id] * padding_length)\n",
    "    \n",
    "    input_ids.append(inputs)\n",
    "    attention_masks.append(mask)\n",
    "    token_type_ids.append(token_type)\n",
    "\n",
    "    return [np.asarray(input_ids), \n",
    "            np.asarray(attention_masks), \n",
    "            np.asarray(token_type_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 529.75it/s]\n",
      "100%|██████████| 40000/40000 [01:13<00:00, 544.31it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_input = text_to_bert_input(X_test)\n",
    "X_train_input = text_to_bert_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the TensorFlow backend to train BERT models, we need to convert the input data into tensors. The following script does that for both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(input_ids,attention_masks,token_type_ids, y):\n",
    "    return {\"input_ids\": input_ids, \n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"token_type_ids\": token_type_ids}, y \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_input[0], X_train_input[1], X_train_input[2]))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2]))\n",
    "#.map(convert_to_tensors).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to define the classification model. With Hugging Face’s transformers library, you can use the TFBertForSequenceClassificaion class to create a text classification model. The process of defining loss, optimizer, and evaluation metrics is similar to any other TensorFlow model. The following script creates a BERT text classification model and displays the model summary in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 502M/502M [06:58<00:00, 1.26MB/s] \n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metric=tf.keras.metrics. SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer,loss=loss,metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 76s 76s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 96s 96s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 69s 69s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 69s 69s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 71s 71s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 69s 69s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 68s 68s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 61s 61s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 62s 62s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 54s 54s/step - loss: 0.0000e+00 - accuracy: 0.9418 - val_loss: 0.0000e+00 - val_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
