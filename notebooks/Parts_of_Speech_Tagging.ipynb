{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS (Parts of Speech) tagging is the task of assigning grammatical classes to words in a natural language sentence based on both its definition and context. POS tagging can be handled either at word-level or sentence-level. At the word-level POS tagging is posed as a classification problem in which an appropriate tag for a word is found whereas at the sentence level a series of tags corresponding to the sequence of words are obtained. The intricacies in POS tagging arise from insufficient training data, inherent POS ambiguities and unknown words which are ubiquitous. POS tagging is an important step in many natural language processing applications such as speech recognition, speech synthesis, information extraction, machine translation, question answering, information retrieval etc. It is also the primary step in the syntactic analysis of a language.\n",
    "\n",
    "Recently, there has been an increase in research on various techniques for “tagging”- assigning a part of speech (or “tag”) to each word in a text. For corpus rich languages like English, there have been many implementations of POS tagger, using machine learning techniques such as Second-Order Markov Models based tagger (Church, 1985) and (Kempe, 1993), Neural network based tagger (Schmid, 1994), Probabilistic Triclass Model-based tagger (Merialdo, 1994), Transformation Based Error-Driven Learning-based tagger (Eric, 1995), Maximum Entropy Markov Model-based tagger (Adwait, 1996), Memory-Based Learning-based tagger (Daelemans and Gillis, 1996), Markov Models based tagger with smoothing techniques and methods to handle unknown words (Brants, 2000).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:** \n",
    "Consider the following sentence '*The can will rust*'. Taking into account the context of the sentence the correct tag for each word is given in bold font along side other possible parts of speech in order of frequency. Individually, '*can*' and '*will*' may be assigned multiple parts of speech.  \n",
    "\n",
    "| The        | can           | will  | rust  |\n",
    "| ------------- |:-------------:|:-------------:| ----------:|\n",
    "| **article**      | modal-verb | **modal-verb** | noun |\n",
    "|      | **noun** | noun | **verb** |\n",
    "|       | verb | verb |       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examplee 2:**\n",
    "Let us look at the following sentence: '*They refuse to permit us to obtain the refuse permit.*'\n",
    "\n",
    "The word refuse is being used twice in this sentence and has two different meanings here. The first refuse is a verb meaning \"*deny*\", while the seconde is a noun meaning \"*trash*\". Thus, the context of the sentence is relevant in determining the part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results provided by the NLTK package, POS tags for both refUSE and REFuse are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most taggers return a single best tag for each word (single taggers). Some work has been done on taggers that return a list of possible tags in those cases where a second (or even third) best choice might be close to the best according to the tagger’s metric (multiple taggers). We might be tempted to ask if using a “multiple tagger”, improves parsing performance. First of all, research has shown that parsing accuracy, as measured by the correct assignment of parts of speech to words, does not significantly increase with multiple tagger. Besides, the work required to parse a sentence goes up with increasing tag ambiguity. Research has shown that single-tag taggers are quite adequate for most task.\n",
    "\n",
    "**Tagging can be used for many NLP tasks like determining correct pronunciation during speech synthesis (for example, dis-count as a noun vs dis-count as a verb), for information retrieval, and for word sense disambiguation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Parts-of-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can be analysed into parts-of-speech, which are major lexical syntactic categories, like\n",
    "- N (Noun)\n",
    "- V (Verb)\n",
    "- A (Adjective)\n",
    "- P (Preposition),\n",
    "\n",
    "or more minor categories, such as \n",
    "- Comp (Complementizer),\n",
    "- Det (Determiner),\n",
    "\n",
    "Some examples are as follows:\n",
    "- **N:** car, cars; woman, women\n",
    "- **V:** thinks, thinking; sold, selling\n",
    "- **A:** old, older, oldest\n",
    "- **P:** in, on, with(out), although\n",
    "- **Comp:** that, if\n",
    "- **Det:** the, a, those, that, some \n",
    "- **Deg:** so, very"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The N, V, A are the categories of the contentful or open-class vocabulary. A glance at any dictionary will tell you that most English words fall in one of these categories. Besides, these categories are open-ended (new words are often invented e.g. fax, biro) and often open-class words belong to more than one category and are thus ambiguous in terms of lexical syntax category (e.g. storm can be a noun or a verb and the morphologically related stormy is an adjective). Adverbs also form a large open-ended class, but they are highly related to adjectives and often formed by adding the suffix *+ly* to adjectives (badly, stormily, etc) so we would not give them a separate category but treat them as A[+Adv].\n",
    "\n",
    "The other categories are those of functional or closed-class words, which typically play a more ‘grammatical’ role with more abstract meaning. This class of words are fewer relative to the above and changes infrequently. For example, prepositions convey some meaning but often this meaning would be indicated by case endings or inflexion on words in other languages and sometimes there are English paraphrases which dispense with the preposition: “Kim gave a cat to Sandy”  or  “Kim gave Sandy a cat.” Degree intensifiers in adjectival or adverbial phrases very beautiful(ly) convey a meaning closely related to the comparative suffix more beautiful / taller. Determiners, such as the (in)definite articles (the, a), demonstrative pronouns (e.g. this, that) or quantifiers (e.g. some, all) help determine the reference of a noun (phrase) – quite frequently articles are absent or indicated morphologically in other languages (hence the common non-native speaker error of the form: “please, where is train station?”). \n",
    "\n",
    "The complete set of lexical syntactic categories in English depends on the syntactic theory, but the smallest sets contain around 20 categories and the largest thousands. Often words are ambiguous between different lexical categories. There are diagnostic rules for determining the category appropriate for a given word in context. For example if a word follows a determiner, it is a noun, e.g., “the song was a hit.” If a word precedes a noun, is not a determiner and modifies the noun’s meaning, it is an adjective e.g. “the smiling boy laughed.” These rules and categorical distinctions can be justified by doing distributional analysis both at the level of words in sentences. The following template schemata are enough to get you to the rules above, which are abstractions based on identifying the classes, like noun, determiner, and adjective  \n",
    "\n",
    "1. – boy(s) can run\n",
    "2. – older boy(s) can run \n",
    "3. The – boy(s) can run \n",
    "4. The older – can run  \n",
    "\n",
    "There are other ways to make these distinctions too. For example, nouns often refer to fairly permanent properties of individuals or objects, boy, car, etc., verbs often denote transitory events or actions, smile, kiss, etc. However, there are many exceptions: storm, philosophy, weigh, believe, etc. Linguists have striven to keep syntax and semantics separate and justify syntactic categories on distributional grounds, but there are many interactions between meaning and syntactic behaviour. \n",
    "There are eight parts of speech (POS) in English: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. The POS are also called word classes, morphological classes, or lexical tags. They are important as they give a significant amount of information about the word and its neighbours. It is true for nouns and verbs. \n",
    "\n",
    "Also, when we have identified, e.g., possessive pronouns my, your, his, her, its and personal pronouns I, he, you, me, we are able to identify the vicinity words. \n",
    "The POS are also used for Information retrieval, as knowing POS can help us as to which morphological affixes it can have. They can also help in selecting important words, like, nouns, from the text. \n",
    "\n",
    "Some examples of POS are as follows:  \n",
    "• **Prepositions:** on, under, over, near, by, at, from, to, with  \n",
    "• **Pronouns:** she, who, I, others  \n",
    "• **Wh-pronouns:** what, who, whom, why, where  \n",
    "• **Conjunctions:** and, but, or, as, if, whom  \n",
    "• **Auxiliary verbs:** can, may, should, is, are  \n",
    "• **Participle:** up, down, on, off, in, out, at, by  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Tag-sets and Parts-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts-of-speech tagging is the process of assigning a parts-of-speech or other lexical class marker to each works in a given text. The figure below depicts the tag-sets used for parts of speech tagging.  \n",
    "<img src=\"img/pos/tag_set.png\" alt=\"Markdown Monster icon\" style=\"float: center; margin-right: 10px;\" />\n",
    "\n",
    "There are various common tagsets for the English language that are used in labelling many corpora. 45-tag Penn Treebank tagset is one such important tagset[1]. This tagset also defines tags for special characters and punctuation apart from other POS tags. The Brown, WSJ, and Switchboard are the three most used tagged corpora for the English language. The Brown corpus consists of a million words of samples taken from 500 written texts in the United States in 1961. The WSJ corpus contains one million words published in the Wall Street Journal in 1989. The Switchboard corpus has twice as many words as Brown corpus. The source of these words is recorded phone conversations between 1990 and 1991. For tagging words from multiple languages, tagset from Nivre et al. [2] is used which is called the Universal POS tagset. This tagset is part of the Universal Dependencies project and contains 16 tags and various features to accommodate different languages. The main application of POS tagging is in sentence parsing, word disambiguation, sentiment analysis, question answering and Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Techniques for POS tagging\n",
    "\n",
    "There are various techniques that can be used for POS tagging such as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Rule-Based POS Tagging\n",
    "\n",
    "The rule-based POS tagging models apply a set of handwritten rules and use contextual information to assign POS tags to words. These rules are often known as context frame rules. One such rule might be: “If an ambiguous/unknown word ends with the suffix ‘ing’ and is preceded by a Verb, label it as a Verb”.\n",
    "\n",
    "The E. Brill’s tagger is one of the first and most widely used English POS-taggers, it employs a rule-based algorithm to assign tags to words. Disambiguation is done by analyzing the linguistic features of the word, its preceding word, its following word, and other aspects. For example, if the preceding word is an article, then the word in question must be a noun. This information is coded in the form of rules.\n",
    "\n",
    "Example of a rule:  \n",
    "*If an ambiguous/unknown word X is preceded by a determiner and followed by a noun, tag it as an adjective.*\n",
    "\n",
    "Brill’s tagger is a rule-based tagger that goes through the training data and finds out the set of tagging rules that best define the data and minimize POS tagging errors. The most important point to note here about Brill’s tagger is that the rules are not hand-crafted, but are instead found out using the corpus provided. The only feature engineering required is a set of rule templates that the model can use to come up with new features.\n",
    "\n",
    "Defining a set of rules manually is an extremely cumbersome process and is not scalable hence the need for more automatic tagging algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Transformation Based Tagging  \n",
    "\n",
    "The transformation-based approaches use a pre-defined set of handcrafted rules as well as automatically induced rules that are generated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Deep learning models \n",
    "Various Deep learning models have been used for POS tagging such as Meta-BiLSTM which have shown an impressive accuracy of around 97 percent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. Stochastic POS Taggers\n",
    "\n",
    "**Stochastic tagger's** refers to any POS tagging model that incorporates a frequency or probabilistic approach in predicting the parts of speech of words in a text.  \n",
    "\n",
    "The simplest stochastic taggers disambiguate words based solely on the probability that a word occurs with a particular tag. In other words, the tag encountered most frequently in the training set with the word is the one assigned to an ambiguous instance of that word. But sometimes this approach comes up with sequences of tags for sentences that are not acceptable according to the grammar rules of a language. One such approach is to calculate the probabilities of various tag sequences that are possible for a sentence and assign the POS tags from the sequence with the highest probability. This is sometimes referred to as the n-gram approach, referring to the fact that the best tag for a given word is determined by the probability that it occurs with the n previous tags. This approach considers the tags for individual words based on context. The next level of complexity that can be introduced into a stochastic tagger uses both the tag sequence probabilities and word frequency measurements. This method uses HMM (Hidden Markov Model) algorithm. Based on this approach we pick up the most likely tag for the given word. \n",
    "\n",
    "Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag. The HMM tagger choose the tag sequence that maximizes the following formula:\n",
    "\n",
    "$$\n",
    "  P(word|tag)∗ P(tag|previous\\text{ n }tags)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.1. Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To undsertand HMM we first take a look at markov models.**\n",
    "\n",
    "Markov models are a type of stochastic model which assume the Markov property i.e. the next state of the system depends only on the present state and not on those preceding it. Thus, to identify a given Markov model, one needs to determine the distribution of probabilities over the initial state *P(S1)* and the *Q × Q* (where Q is the number of states) matrix of probabilities of going from each state to all states including itself.\n",
    "\n",
    "The initial state probabilities are\n",
    "\n",
    "$$\n",
    "π_{i}=P(q_{1}=S_{i}), \\text{  } i=1,…,Q\n",
    "$$\n",
    "\n",
    "and the state transition probabilities are\n",
    "$$\n",
    "α_{i,j}=P(q_{t}=j| q_{t-1}=i), \\text{    } i,j=1,…,Q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a vocabulary of 3 words, namely \n",
    "- part (Sunny)\n",
    "- speech (Rainy)\n",
    "- tag (Cloudy)\n",
    "\n",
    "Assume we have a sequence of text from our vocabulary\n",
    "\n",
    "*part, speech, tag, tag, part, part, part, speech*\n",
    "\n",
    "Let’s say we decide to use a Markov Chain Model to predict the next word in the sequence. Using the data that we have, we can construct the following state diagram with the labelled probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pos/mm_diag.png\" alt=\"Markdown Monster icon\" style=\"float: left; margin-right: 10px;\" />\n",
    "$$  \n",
    "  P(Part|Part) = 0.8  \n",
    "$$\n",
    "$$ \n",
    "  P(Speech|Part) = 0.05\n",
    "$$\n",
    "$$\n",
    "  P(Tag|Part) = 0.15       \n",
    "$$\n",
    "  \n",
    "  \n",
    "$$ \n",
    "  P(Speech|Speech) = 0.2\n",
    "$$\n",
    "$$ \n",
    "  P(Part|Speech) = 0.6\n",
    "$$ \n",
    "$$ \n",
    "  P(Tag|Speech) = 0.2      \n",
    "$$\n",
    "  \n",
    "  \n",
    "$$ \n",
    "  P(Tag|Tag) = 0.2 \n",
    "$$\n",
    "$$ \n",
    "  P(Speech|Tag) = 0.3\n",
    "$$\n",
    "$$\n",
    "  P(Part|Tag) = 0.5  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the probability of the next word given N previous observations, we will use the Markovian Property.  \n",
    "<img src=\"img/pos/mm_prop.png\" alt=\"Markovian property\" style=\"float: center; margin-right: 10px;\" />\n",
    "\n",
    "**Exercise 1:** Given that the current word in the sequence is *part* what is the probability that the next word is *part* and the next is *tag*  \n",
    "\n",
    "$\n",
    "P(q_{2}, q_{3} | q_{1}) = P(q_{2}|q_{1})*P(q_{3}| q_{2}, q_{1})  \n",
    "$\n",
    "\n",
    "$\n",
    "= P(q_{2}|q_{1})*P(q_{3}| q_{2})  \n",
    "$\n",
    "\n",
    "$\n",
    "= P(part|part)*P(tag|part)  \n",
    "$\n",
    "\n",
    "$\n",
    "= (0.8)*P(0.05)  \n",
    "$\n",
    "\n",
    "$\n",
    "= 0.04  \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.2. Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hidden Markov Models (HMM) is a statistical model for modelling generative sequences characterized by an underlying process generating an observable sequence. HMMs are based on Markov chains. A Markov chain is a model that describes a sequence of potential events in which the probability of an event is dependant only on the state which is attained in the previous event. The Markov model is based on a Markov assumption in predicting the probability of a sequence. If state variables are defined as \n",
    "$$\n",
    "q_{1}, q_{2}, ..., q_{n}\n",
    "$$, \n",
    "\n",
    "The Markov assumption is defined as  \n",
    "$$\n",
    "P( q_{n} = a| q_{1}, q_{2},..,q_{n-1}) = P(q_{n} = a|q_{n-1})\n",
    "$$\n",
    "\n",
    "An HMM consists of two components, the A and the B probabilities. The A matrix contains the tag transition probabilities and B the emission probabilities. The transition probability, given a tag, how often is this tag is followed by the second tag in the corpus is calculated by:\n",
    "\n",
    "$$\n",
    "P(t_{i}|t_{i-1}) = \\frac{C(t_{i-1}, t_{i})}{C(t_{i-1})}\n",
    "$$\n",
    "\n",
    "The emission probability, given a tag, how likely it will be associated with a word is given by:\n",
    "\n",
    "$$\n",
    "P(w_{i}|t_{i}) = \\frac{C(t_{i}, w_{i})}{C(t_{i})}\n",
    "$$\n",
    "\n",
    "where w denotes the word and t denotes the tag.\n",
    "\n",
    "**HMM Tagger**\n",
    "\n",
    "The process of determining hidden states to their corresponding sequence is known as decoding. More formally, given A, B probability matrices and a sequence of observations O\n",
    "$$\n",
    "O = o_{1}, o_{2}, ..., o_{T}\n",
    "$$\n",
    "the goal of an HMM tagger is to find a sequence of states S\n",
    "$$\n",
    "S = s_{1}, s_{2}, ..., s_{T}\n",
    "$$\n",
    "For POS tagging the task is to find a tag sequence *t* that maximizes the probability of a sequence of observations of *n* words *w*\n",
    "\n",
    "<img src=\"img/pos/hmm_max.svg\" alt=\"Markovian property\" style=\"float: center; margin-left: 300px;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how HMM selects an appropriate tag sequence for a sentence let's consider an example proposed by Dr Luis Serrano.\n",
    "\n",
    "The sentence “*Ted will spot Will.*”  can be tagged as a noun, model, verb and noun. To calculate the probability associated with this particular sequence of tags we require their Transition and Emission probabilities.\n",
    "\n",
    "The transition probability is the likelihood of a particular sequence (e.g. example, how likely is a noun followed by a model and a model by a verb and a verb by a noun). \n",
    "\n",
    "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of probabilities are Emission probabilities and should be high for our tagging to be likely.\n",
    "\n",
    "Let us calculate the above two probabilities for the set of sentences below\n",
    "\n",
    "    Mary Jane can see Will\n",
    "    Spot will see Mary\n",
    "    Will Jane spot Mary?\n",
    "    Mary will pat Spot\n",
    "\n",
    "Note that Mary Jane, Spot, and Will are all names.  \n",
    "\n",
    "<img src=\"img/pos/pos3-1.png\" alt=\"\" style=\"float: center; margin-left: 50px;\" />\n",
    "\n",
    "To calculate the emission probabilities, we first create a counting table.\n",
    "\n",
    "<img src=\"img/pos/count_matrix.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "Next we divide each column by the total number of their appearances for example, 'noun' appears nine times in the above sentences so divide each term by 9 in the noun column.\n",
    "\n",
    "<img src=\"img/pos/count_matrix_2.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "From the above table, we infer that  \n",
    "\n",
    "The probability that Mary is Noun = 4/9  \n",
    "The probability that Mary is Model = 0  \n",
    "The probability that Will  is Noun = 1/9  \n",
    "The probability that Will is Model = 3/4  \n",
    "\n",
    "In a similar manner, you can figure out the rest of the probabilities. These are the emission probabilities. Next, we have to calculate the transition probabilities, so define two more tags \\<S> and \\<E>. \\<S> is placed at the beginning of each sentence and \\<E> at the end as shown in the figure below.\n",
    "\n",
    "<img src=\"img/pos/pos4.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "Let us again create a table and fill it with the co-occurrence counts of the tags.\n",
    "\n",
    "<img src=\"img/pos/emission.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "In the above figure, we can see that the \\<S> tag is followed by the N tag three times, thus the first entry is 3.The model tag follows the \\<S> just once, thus the second entry is 1. In a similar manner, the rest of the table is filled.\n",
    "\n",
    "Next, we divide each term in a row of the table by the total number of co-occurrences of the tag in consideration, for example, The Model tag is followed by any other tag four times as shown below,\n",
    "    \n",
    "<img src=\"img/pos/pos-5.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "thus we divide each element in the third row by four. These are the respective transition probabilities for the above four sentences.    \n",
    "\n",
    "<img src=\"img/pos/emission_2.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "To use the HMM to determine the appropriate sequence of tags for a particular sentence say '*Will can spot Mary*’. Let's tag the word with the wrong tags:  \n",
    "    \n",
    "    Will as a  model\n",
    "    Can as a verb\n",
    "    Spot as a noun\n",
    "    Mary as a noun\n",
    "\n",
    "Now calculate the probability of this sequence being correct in the following manner.\n",
    "\n",
    "<img src=\"img/pos/pos6-1.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "    \n",
    "The probability of the tag Model (M) comes after the tag \\<S> is ¼ as seen in the table. Also, the probability that the word Will is a Model is 3/4. In the same manner, we calculate each and every probability in the graph. Now the product of these probabilities is the likelihood that this sequence is right. Since the tags are not correct, the product is zero.\n",
    "$$\n",
    "\\frac{1}{4}*\\frac{3}{4}*\\frac{3}{4}*0*1*\\frac{2}{9}*\\frac{1}{9}*\\frac{4}{9}*\\frac{4}{9}=0\n",
    "$$\n",
    "\n",
    "When these words are correctly tagged, we get a probability greater than zero as shown below\n",
    "\n",
    "<img src=\"img/pos/pos7.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "Calculating  the product of these terms we get,\n",
    "\n",
    "$$\n",
    "\\frac{3}{4}*\\frac{1}{9}*\\frac{3}{9}*\\frac{1}{4}*\\frac{3}{4}*\\frac{1}{4}*1*\\frac{4}{9}*\\frac{4}{9}=0.00025720164\n",
    "$$\n",
    "\n",
    "\n",
    "Form the three POS tags mentioned above, 81 different combinations of tags can be formed. Now let us visualize these 81 combinations as paths and using the transition and emission probability mark each vertex and edge as shown below.\n",
    "\n",
    "<img src=\"img/pos/pos8.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "The next step is to delete all the vertices and edges with probability zero, also the vertices which do not lead to the endpoint are removed.  \n",
    "\n",
    "<img src=\"img/pos/pos9.png\" alt=\"\" style=\"float: center; margin-left: 10px;\" />\n",
    "\n",
    "Now there are only two paths that lead to the end, let us calculate the probability associated with each path.\n",
    "\n",
    "$$\n",
    "<S>→N→M→N→N→<E> = \n",
    "\\frac{3}{4}*\\frac{1}{9}*\\frac{3}{9}*\\frac{1}{4}*\\frac{1}{4}*\\frac{2}{9}*\\frac{1}{9}*\\frac{4}{9}*\\frac{4}{9}=0.00000846754\n",
    "$$\n",
    "\n",
    "$$\n",
    "<S>→N→M→N→V→<E> = \n",
    "\\frac{3}{4}*\\frac{1}{9}*\\frac{3}{9}*\\frac{1}{4}*\\frac{3}{4}*\\frac{1}{4}*1*\\frac{4}{9}*\\frac{4}{9}=0.00025720164\n",
    "$$\n",
    "\n",
    "The second sequence have the larger probability and hence will be selected by the HMM to tag each word in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.3. Example\n",
    "Consider the following example sentences:  \n",
    "\n",
    "    Secretariat is expected to race tomorrow.  \n",
    "    People continue to inquire the reason for the race for outer space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Secretariat', 'NNP'), ('is', 'VBZ'), ('expected', 'VBN'), ('to', 'TO'), ('race', 'NN'), ('tomorrow', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence_1 = \"Secretariat is expected to race tomorrow\"\n",
    "tokens_1 = nltk.word_tokenize(sentence_1)\n",
    "print(nltk.pos_tag(tokens_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('People', 'NNS'), ('continue', 'VBP'), ('to', 'TO'), ('inquire', 'VB'), ('the', 'DT'), ('reason', 'NN'), ('for', 'IN'), ('the', 'DT'), ('race', 'NN'), ('for', 'IN'), ('outer', 'JJ'), ('space', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence_2 = \"People continue to inquire the reason for the race for outer space\"\n",
    "tokens_2 = nltk.word_tokenize(sentence_2)\n",
    "print(nltk.pos_tag(tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**  \n",
    "[1] M. MARCUS, B. SANTORINI, and M. MARCINKIEWICZ, “Building a Large Annotated Corpus of English: The Penn Treebank,” Comput. Linguist., 1993.  \n",
    "[2] J. Nivre et al., “Universal dependencies v1: A multilingual treebank collection,” in Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016, 2016.  \n",
    "[3] M. J. H. Jurafsky, Daniel, “Speech and Language Processing 3rd Edition Draft.” [Online](https://web.stanford.edu/~jurafsky/slp3/).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
